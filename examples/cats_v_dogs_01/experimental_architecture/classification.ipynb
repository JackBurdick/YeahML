{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 6, 5, 'final', 0)\n",
      "TensorFlow: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "import random\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors # making colors consistent\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "\n",
    "# from imageio import imread # read image from disk\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "import pickle # manually saving best params\n",
    "from sklearn.utils import shuffle # shuffling data batches\n",
    "from tqdm import tqdm # display training progress bar\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "def make_standard_dirs(root=\"trial\", saver=True, best_params=True, tf_logs=True):\n",
    "    # `saver/` will hold tf saver files\n",
    "    maybe_create_dir(root + \"/saver\")\n",
    "    # `best_params/` will hold a serialized version of the best params\n",
    "    # I like to keep this as a backup in case I run into issues with\n",
    "    # the saver files\n",
    "    maybe_create_dir(root + \"/best_params\")\n",
    "    # `tf_logs/` will hold the logs that will be visable in tensorboard\n",
    "    maybe_create_dir(root + \"/tf_logs\")\n",
    "\n",
    "    \n",
    "# set tf log level to supress messages, unless an error\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial/saver already exists\n",
      "trial/best_params already exists\n",
      "trial/tf_logs already exists\n"
     ]
    }
   ],
   "source": [
    "make_standard_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.tfrecords\n",
      "train.tfrecords\n",
      "val.tfrecords\n"
     ]
    }
   ],
   "source": [
    "BEST_PARAMS_PATH = \"best_params\"\n",
    "TFR_DIR = \"./data/record_holder/150\"\n",
    "for _, _, files in os.walk(TFR_DIR):\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these two functions (get_model_params and restore_model_params) are \n",
    "# ad[a|o]pted from; \n",
    "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "def get_model_params():\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {global_vars.op.name: value for global_vars, value in \n",
    "            zip(global_vars, tf.get_default_session().run(global_vars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "# these two functions are used to manually save the best\n",
    "# model params to disk\n",
    "def save_obj(obj, name):\n",
    "    with open('trial/best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('trial/best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SET_TYPE = None\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    global GLOBAL_SET_TYPE\n",
    "    labelName = str(GLOBAL_SET_TYPE) + '/label'\n",
    "    featureName = str(GLOBAL_SET_TYPE) + '/image'\n",
    "    feature = {featureName: tf.FixedLenFeature([], tf.string),\n",
    "               labelName: tf.FixedLenFeature([], tf.int64)}\n",
    "    \n",
    "    # decode\n",
    "    parsed_features = tf.parse_single_example(example_proto, features=feature)\n",
    "    \n",
    "    # convert image data from string to number\n",
    "    image = tf.decode_raw(parsed_features[featureName], tf.float32)\n",
    "    image = tf.reshape(image, [150, 150, 3])\n",
    "    label = tf.cast(parsed_features[labelName], tf.int64)\n",
    "    \n",
    "    # [do any preprocessing here]\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_batched_iter(setType, data_params, sess):\n",
    "    global GLOBAL_SET_TYPE\n",
    "    global TFR_DIR\n",
    "    GLOBAL_SET_TYPE = setType\n",
    "    \n",
    "    filenames_ph = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames_ph)\n",
    "    dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "    if GLOBAL_SET_TYPE != 'test':\n",
    "        dataset = dataset.shuffle(buffer_size=data_params['buffer_size'])\n",
    "    #dataset = dataset.shuffle(buffer_size=1)\n",
    "    dataset = dataset.batch(data_params['batch_size'])\n",
    "    dataset = dataset.repeat(1)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "    tfrecords_file_name = str(GLOBAL_SET_TYPE) + '.tfrecords'\n",
    "    tfrecord_file_path = os.path.join(TFR_DIR, tfrecords_file_name)\n",
    "    \n",
    "    # initialize\n",
    "    sess.run(iterator.initializer, feed_dict={filenames_ph: [tfrecord_file_path]})\n",
    "    \n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyper_params():\n",
    "    data_params = {}\n",
    "    data_params['n_epochs'] = 5\n",
    "    data_params['batch_size'] = 32\n",
    "    data_params['buffer_size'] = 128 # for shuffling\n",
    "\n",
    "    data_params['init_lr'] = 1e-5\n",
    "\n",
    "    return data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data_params):\n",
    "    g = tf.Graph()\n",
    "    n_outputs = 1\n",
    "    IMG_HEIGHT = 150\n",
    "    IMG_WIDTH = 150\n",
    "    CHANNELS = 3\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            X = tf.placeholder(tf.float32, shape=[None, IMG_HEIGHT, IMG_WIDTH, CHANNELS], name=\"X\") # Input\n",
    "            #y = tf.placeholder(tf.float32, shape=(None, n_outputs), name=\"labels\") # Target\n",
    "            y_raw = tf.placeholder(tf.int64, shape=[None, n_outputs], name=\"y_input\")\n",
    "            y = tf.cast(y_raw, tf.float32, name=\"label\")\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            #224x224x3\n",
    "            h_1 = tf.layers.conv2d(X, filters=32, kernel_size=3, activation=tf.nn.elu,\n",
    "                                   padding='SAME', strides=2, name=\"conv_1\") # 112x112x3\n",
    "            \n",
    "            #112x112x32\n",
    "            h_2 = tf.layers.conv2d(h_1, filters=64, kernel_size=3, activation=tf.nn.elu,\n",
    "                                   padding='SAME', strides=2, name=\"conv_2\") # 64x64x64\n",
    "            \n",
    "            #64x64x64\n",
    "            h_3 = tf.layers.conv2d(h_2, filters=96, kernel_size=3, activation=tf.nn.elu,\n",
    "                                   padding='SAME', strides=2, name=\"conv_3\") # 32x32x96\n",
    "            \n",
    "            # 32x32x96\n",
    "            h_4 = tf.layers.max_pooling2d(h_3, pool_size=[2,2],\n",
    "                                          strides=2, name=\"max_pool_01\") # 16x16x96\n",
    "            \n",
    "            # 16x16x96\n",
    "            h_5 = tf.layers.conv2d(h_4, filters=128, kernel_size=3, activation=tf.nn.elu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_4\") # 16x16x128\n",
    "            \n",
    "            # 16x16x128\n",
    "            h_6 = tf.layers.conv2d(h_5, filters=192, kernel_size=3, activation=tf.nn.elu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_5\") # 16x16x192\n",
    "            \n",
    "            # 16x16x192\n",
    "#             h_7 = tf.layers.max_pooling2d(h_6, pool_size=[2,2],\n",
    "#                                           strides=2, name=\"max_pool_02\") # 8x8x192\n",
    "            \n",
    "#             # 8x8x192\n",
    "#             h_8 = tf.layers.conv2d(h_7, filters=256, kernel_size=3, activation=tf.nn.elu,\n",
    "#                                    padding='SAME', strides=2, name=\"conv_6\") # 4x4x256\n",
    "            \n",
    "#             # 4x4x256\n",
    "#             h_9 = tf.layers.conv2d(h_8, filters=1024, kernel_size=4, activation=tf.nn.elu,\n",
    "#                                    padding='SAME', strides=1, name=\"conv_7\") # 1x1x1024\n",
    "            last_shape = int(np.prod(h_6.get_shape()[1:]))\n",
    "            h_out_flat = tf.reshape(h_6, shape=[-1, last_shape]) # 1024\n",
    "            \n",
    "            # 1024\n",
    "            h_10 = tf.layers.dense(h_out_flat, 256, name=\"layer_01\", activation=tf.nn.elu)\n",
    "            h_11 = tf.layers.dense(h_10, 64, name=\"layer_02\", activation=tf.nn.elu)\n",
    "            h_12 = tf.layers.dense(h_11, 16, name=\"layer_03\", activation=tf.nn.elu)\n",
    "            \n",
    "            logits = tf.layers.dense(h_12, n_outputs, name=\"logits\")\n",
    "            preds = tf.sigmoid(logits, name=\"preds\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            batch_loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=data_params['init_lr'],\n",
    "                                               beta1=0.9,\n",
    "                                               beta2=0.999,\n",
    "                                               epsilon=1e-08,\n",
    "                                               use_locking=False,\n",
    "                                               name='Adam')\n",
    "            training_op = optimizer.minimize(batch_loss, name=\"training_op\")\n",
    "            \n",
    "        with tf.name_scope(\"save_session\"):\n",
    "            init_global = tf.global_variables_initializer()\n",
    "            init_local = tf.local_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        # Ops: training metrics\n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            # ================================== performance\n",
    "            with tf.name_scope(\"common\"):\n",
    "                #preds = tf.nn.softmax(logits, name=\"prediction\")\n",
    "                #y_true_cls = tf.argmax(y,1)\n",
    "                #y_pred_cls = tf.argmax(preds,1)\n",
    "                y_true_cls = tf.greater_equal(y, 0.5)\n",
    "                y_pred_cls = tf.greater_equal(preds, 0.5)\n",
    "\n",
    "                correct_prediction = tf.equal(y_pred_cls, y_true_cls, name=\"correct_predictions\")\n",
    "                batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            with tf.name_scope(\"train_metrics\") as scope:\n",
    "                train_auc, train_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                train_acc, train_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_met_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_met_reset_op\")\n",
    "            with tf.name_scope(\"val_metrics\") as scope:\n",
    "                val_auc, val_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                val_acc, val_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_met_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_met_reset_op\")\n",
    "            with tf.name_scope(\"test_metrics\") as scope:\n",
    "                test_auc, test_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                test_acc, test_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_met_reset_op\")\n",
    "\n",
    "            # =============================================== loss \n",
    "            with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "                train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "            with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "                val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "            with tf.name_scope(\"test_loss_eval\")as scope:\n",
    "                test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")\n",
    "\n",
    "        # --- create collections\n",
    "        for node in (saver, init_global, init_local):\n",
    "            g.add_to_collection(\"save_init\", node)\n",
    "        for node in (X, y_raw, training_op):\n",
    "            g.add_to_collection(\"main_ops\", node)\n",
    "        for node in (preds, y_true_cls, y_pred_cls, correct_prediction):\n",
    "            g.add_to_collection(\"preds\", node)\n",
    "        for node in (train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op):\n",
    "            g.add_to_collection(\"train_metrics\", node)\n",
    "        for node in (val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op):\n",
    "            g.add_to_collection(\"val_metrics\", node)\n",
    "        for node in (test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op):\n",
    "            g.add_to_collection(\"test_metrics\", node)\n",
    "        for node in (train_mean_loss, train_mean_loss_update, train_loss_reset_op):\n",
    "            g.add_to_collection(\"train_loss\", node)\n",
    "        for node in (val_mean_loss, val_mean_loss_update, val_loss_reset_op):\n",
    "            g.add_to_collection(\"val_loss\", node)\n",
    "        for node in (test_mean_loss, test_mean_loss_update, test_loss_reset_op):\n",
    "            g.add_to_collection(\"test_loss\", node)\n",
    "        g.add_to_collection(\"logits\", logits)\n",
    "            \n",
    "        # ===================================== tensorboard\n",
    "        with tf.name_scope(\"tensorboard_writer\") as scope:\n",
    "            epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "            epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_acc)\n",
    "            epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "            epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_acc_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "            # ===== epoch, validation\n",
    "            epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "            epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_acc)\n",
    "            epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "            epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_acc_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "        \n",
    "        for node in (epoch_train_write_op, epoch_validation_write_op):\n",
    "            g.add_to_collection(\"tensorboard\", node)\n",
    "            \n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(g):\n",
    "    global BEST_PARAMS_PATH\n",
    "    saver, init_global, init_local = g.get_collection(\"save_init\")\n",
    "    X, y_raw, training_op = g.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls, _ = g.get_collection(\"preds\")\n",
    "    train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op = g.get_collection(\"train_metrics\")\n",
    "    val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op = g.get_collection(\"val_metrics\")\n",
    "    train_mean_loss, train_mean_loss_update, train_loss_reset_op = g.get_collection(\"train_loss\")\n",
    "    val_mean_loss, val_mean_loss_update, val_loss_reset_op = g.get_collection(\"val_loss\")\n",
    "    epoch_train_write_op, epoch_validation_write_op = g.get_collection(\"tensorboard\")\n",
    "#     next_tr_element, next_val_element, _ = g.get_collection(\"data_sets\")\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"train\"))\n",
    "    val_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"validation\"))\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    with tf.Session(graph=g) as sess:\n",
    "\n",
    "        # test\n",
    "#         test_iter = return_batched_iter('test', data_params, sess)\n",
    "#         next_test_element = test_iter.get_next()\n",
    "        sess.run([init_global, init_local])\n",
    "        \n",
    "        for e in tqdm(range(1,data_params['n_epochs']+1)):\n",
    "            sess.run([val_met_reset_op,val_loss_reset_op,train_met_reset_op,train_loss_reset_op])\n",
    "            # training\n",
    "            tr_iter = return_batched_iter('train', data_params, sess)\n",
    "            next_tr_element = tr_iter.get_next()\n",
    "            \n",
    "            # loop entire training set\n",
    "            while True:\n",
    "                try:\n",
    "                    data, target = sess.run(next_tr_element)\n",
    "                    target = np.reshape(target, (target.shape[0], 1))\n",
    "                    sess.run([training_op, train_auc_update, train_acc_update, train_mean_loss_update], \n",
    "                             feed_dict={X:data, y_raw:target})\n",
    "#                     pr, yt, yp = sess.run([preds, y_true_cls, y_pred_cls], feed_dict={X:data, y_raw:target})\n",
    "#                     print(pr)\n",
    "#                     print(yt)\n",
    "#                     print(yp)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        \n",
    "            # write average for epoch\n",
    "            summary = sess.run(epoch_train_write_op)    \n",
    "            train_writer.add_summary(summary, e)\n",
    "            train_writer.flush()\n",
    "\n",
    "            # run validation\n",
    "            # validation\n",
    "            val_iter = return_batched_iter('val', data_params, sess)\n",
    "            next_val_element = val_iter.get_next()\n",
    "            while True:\n",
    "                try:\n",
    "                    Xb, yb = sess.run(next_val_element)\n",
    "                    yb = np.reshape(yb, (yb.shape[0], 1))\n",
    "                    sess.run([val_auc_update, val_acc_update, val_mean_loss_update], feed_dict={X:Xb, y_raw:yb})\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "\n",
    "            # check for (and save) best validation params here\n",
    "            cur_loss, cur_acc = sess.run([val_mean_loss, val_acc])\n",
    "            if cur_loss < best_val_loss:\n",
    "                best_val_loss = cur_loss\n",
    "                best_params = get_model_params()\n",
    "                save_obj(best_params, BEST_PARAMS_PATH)\n",
    "                print(\"best params saved: val acc: {:.3f}% val loss: {:.4f}\".format(cur_acc*100, cur_loss))\n",
    "\n",
    "            summary = sess.run(epoch_validation_write_op) \n",
    "            val_writer.add_summary(summary, e)\n",
    "            val_writer.flush()\n",
    "        \n",
    "        train_writer.close()\n",
    "        val_writer.close()\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:17<01:09, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 66.875% val loss: 0.6003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:32<00:48, 16.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 67.800% val loss: 0.5869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:48<00:32, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 72.025% val loss: 0.5530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [01:03<00:15, 15.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 73.250% val loss: 0.5433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 5/5 [01:18<00:00, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 72.400% val loss: 0.5393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g = build_graph(data_params)\n",
    "sess = train_graph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 80.719% acc: 72.380% loss: 0.54868\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g2 = build_graph(data_params)\n",
    "best_params = load_obj(BEST_PARAMS_PATH)\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    saver, init_global, init_local = g2.get_collection(\"save_init\")\n",
    "    X, y_raw, training_op = g2.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls, _ = g2.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g2.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g2.get_collection(\"test_loss\")\n",
    "    \n",
    "    restore_model_params(model_params=best_params, g=g2, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    \n",
    "    test_iter = return_batched_iter('test', data_params, sess)\n",
    "    next_test_element = test_iter.get_next()\n",
    "    while True:\n",
    "        try:\n",
    "            Xb, yb = sess.run(next_test_element)\n",
    "            yb = np.reshape(yb, (yb.shape[0], 1))\n",
    "            sess.run([test_auc_update, test_acc_update, test_mean_loss_update], feed_dict={X:Xb, y_raw:yb})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break    \n",
    "    \n",
    "    # print\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                              final_test_acc*100,\n",
    "                                                              final_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_edge",
   "language": "python",
   "name": "dl_edge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
