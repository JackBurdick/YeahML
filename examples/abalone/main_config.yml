meta:
  data_name: 'abalone'
  experiment_name: 'trial_00'
  # TODO: information on when to save params, currently only best params saved
logging:
  console:
    level: 'info'
    format_str: null
  file:
    level: 'ERROR'
    format_str: null
  graph_spec: True

performance:
  objectives:
    main:
      loss: 
        type: 'MSE'
        #options:
      metric:
        type: ["MeanSquaredError", "MeanAbsoluteError"]
        options: [null, 
                  null]
      in_config:
        type: "supervised"
        options:
          prediction: "dense_out"
          target: "target_v"

# TODO: this section needs to be redone
data:
  in:
    features:
      shape: [2,1]
      dtype: 'float64'
    target_v:
      shape: [1]
      dtype: 'int32'
      endpoint: True # TODO: may modify this eventually

hyper_parameters:
  optimizer: 
    type: 'adam'
    options:
      learning_rate: 0.0001
      beta_1: 0.91
  epochs: 30
  dataset:
    # TODO: I would like to make this logic more abstract
    # I think the only options that should be applied here are "batch" and "shuffle"
    batch: 16
    shuffle_buffer: 128 # this should be grouped with batchsize
model:
  path: './model_config.yml'

