meta:
  data_name: 'abalone'
  experiment_name: 'trial_00'
  # TODO: information on when to save params, currently only best params saved
logging:
  console:
    level: 'info'
    format_str: null
  file:
    level: 'ERROR'
    format_str: null
  graph_spec: True

performance:
  objectives:
    # each objective can be a loss, a metric, or a combination of both
    main_obj:
      loss:
        # each loss can only be one loss
        # TODO: support custom losses
        type: 'MSE'
        track: "mean"
        #options:
      metric:
        # there can be many metrics
        # TODO: support custom metrics
        type: ["MeanSquaredError"]
        options: [null]
      in_config:
        type: "supervised"
        options:
          prediction: "dense_out"
          target: "target_v"
    second_obj:
      loss: 
        type: 'MAE'
        track: "mean"
        #options:
      metric:
        type: ["meanabsoluteerror"]
        options: [null]
      in_config:
        type: "supervised"
        options:
          prediction: "dense_out"
          target: "target_v"

# TODO: this section needs to be redone
data:
  in:
    feature_a:
      shape: [2,1]
      dtype: 'float64'
    target_v:
      shape: [1,1]
      dtype: 'int32'
      label: True

optimize:
  # NOTE: multiple losses by the same optimizer, are currently only modeled
  # jointly, if we wish to model the losses seperately (sequentially or
  # alternating), then we would want to use a second optimizer
  optimizers:
    "main_opt":
      type: 'adam'
      options:
        learning_rate: 0.0001
        beta_1: 0.91 
      objectives: ["main_obj"]
    "second_opt":
      type: 'adam'
      options:
        learning_rate: 0.0001
        beta_1: 0.91 
      objectives: ["second_obj"]
  directive:
    # (a|b): alternate, (a&b): joint, (a,b): sequential
    # eventually, we should be able to specify time/epoch/% before moving to the
    # next instruction, but presently each `opt` represents an (ENTIRE) pass of the training set
    # instruct: "(a|b)"
    instructions: "(((main_opt,second_opt)&second_opt)&second_opt)"

hyper_parameters:
  # optimizer_definition:
  #   seq: "(((a|b)&c),d)"
  # optimizer: # TODO: support multiple optimizers
  #   name: "main_2"
  #   type: 'adam'
  #   options:
  #     learning_rate: 0.001
  #     beta_1: 0.92
  epochs: 30
  dataset:
    # TODO: I would like to make this logic more abstract
    # I think the only options that should be applied here are "batch" and "shuffle"
    batch: 16
    shuffle_buffer: 128 # this should be grouped with batchsize
  # other:
  #   earlystopping:
  #     type: 'earlystopping'
  #     options:

model:
  path: './model_config.yml'

