meta:
  name: "model_a"
  name_override: True
  activation:
    type: 'elu'

# TODO: it could be assumed that if only one layer in is defined in main, that
# it could be the `in_name` to the first layer.
layers:
  flatten_1:
    type: 'flatten'
    in_name: 'feature_a'
  dense_1:
    type: 'dense'
    options:
      units: 16
    # in_name: 'feature_a'
  dense_2:
    type: 'dense'
    options:
      units: 8
      activation:
        type: 'elu'
  dense_2b:
    type: 'dense'
    options:
      units: 8
      activation:
        type: 'elu'
    in_name: 'dense_1'
  concat_1:
    type: 'concatenate'
    in_name: ['dense_2', 'dense_2b']
  dense_3a:
    type: 'dense'
    options:
      units: 8
      activation:
        type: 'elu'
  dense_3b:
    type: 'dense'
    options:
      units: 8
      activation:
        type: 'elu'
    in_name: 'dense_2'
  concat_3:
    type: 'concatenate'
    in_name: ['dense_3a', 'dense_3b']
  dense_out:
    type: 'dense'
    options:
      units: 1
      activation:
        type: 'linear'
    endpoint: True

# TODO: need to support this case
  # dense_3_a:
  #   type: 'dense'
  #   options:
  #     units: 2
  #     activation:
  #       type: 'elu'
  # dense_3_b:
  #   type: 'dense'
  #   options:
  #     units: 2
  #     activation:
  #       type: 'elu'
  # dense_4:
  #   type: 'concatenate'
  #   options:
  #     axis: -1
  #   in_name: ['dense_3_a', 'dense_3_b']
  # dense_out:
  #   type: 'dense'
  #   options:
  #     units: 1
  #     activation:
  #       type: 'linear'


    # "layers": {
    #     "dense_1": {
    #         "type": "dense",
    #         "options": {
    #             "units": "16",
    #             "kernel_initializer": {"type": "glorotnormal"},
    #             "bias_regularizer": {"type": "l2", "options": {"l": 0.3}},
    #             "activation": {"type": "elu"},
    #         },
    #         "in_name": "jack",
    #     },
    #     "bn_1": {
    #         "type": "batchnormalization",
    #         "options": None,
    #         "in_name": "dense_1",
    #     },
    #     "dense_2": {
    #         "type": "dense",
    #         "options": {
    #             "units": "16",
    #             "kernel_initializer": {"type": "glorotnormal"},
    #             "bias_regularizer": {"type": "l2", "options": {"l": 0.3}},
