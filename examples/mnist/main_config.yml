meta:
  data_name: 'mnist'
  experiment_name: 'trial_00'
  # TODO: information on when to save params, currently only best params saved
logging:
  console:
    level: 'info'
    format_str: null
  file:
    level: 'ERROR'
    format_str: null
  graph_spec: True

performance:
  objectives:
    # each objective can be a loss, a metric, or a combination of both
    main_obj:
      loss:
        # each loss can only be one loss
        # TODO: support custom losses
        # TODO: CategoricalCrossentropy does not work. 
        type: 'Categorical_Crossentropy'
        track: "mean"
        #options:
      metric:
        # there can be many metrics
        # TODO: support custom metrics
        type: ["CategoricalAccuracy"]
        options: [null]
      in_config:
        type: "supervised"
        options:
          prediction: "y_pred"
          target: "y_target"
    second_obj:
      metric:
        type: ["TopKCategoricalAccuracy", "TopKCategoricalAccuracy"]
        options: [{"k": 2}, {"k": 3}]
      in_config:
        type: "supervised"
        options:
          prediction: "y_pred"
          target: "y_target"

# TODO: this section needs to be redone
data:
  in:
    x_image:
      shape: [28,28,1]
      dtype: 'float32' # this is a cast
    y_target:
      shape: [1,1]
      dtype: 'int32'
      label: True

optimize:
  # NOTE: multiple losses by the same optimizer, are currently only modeled
  # jointly, if we wish to model the losses seperately (sequentially or
  # alternating), then we would want to use a second optimizer
  optimizers:
    "main_opt":
      type: 'adam'
      options:
        learning_rate: 0.001
      objectives: ["main_obj"]

hyper_parameters:
  epochs: 15
  dataset:
    # TODO: I would like to make this logic more abstract
    # I think the only options that should be applied here are "batch" and "shuffle"
    batch: 16
    shuffle_buffer: 128 # this should be grouped with batchsize
  # other:
  #   earlystopping:
  #     type: 'earlystopping'
  #     options:

model:
  path: './model_config.yml'

